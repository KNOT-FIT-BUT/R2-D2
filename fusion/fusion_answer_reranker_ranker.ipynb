{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ifajcik/efficientqa_pruned/efficientQA_submission'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_abstractive_reader_outputs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_fused.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonlfused_w_abs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_gen_reranked_reader_outputs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_preprocessed_queryenc_wikipassages_NQ_open_val_<class 'transformers.tokenization_roberta.RobertaTokenizer'>.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_ranked_outputs.jsonl\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reader_outputs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reranked_outputs.jsonl\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reranked_outputs.jsonl_fuseindecoder_preprocessed_for_C25_t5-base_L512.json\n"
     ]
    }
   ],
   "source": [
    "! ls | grep 'NQ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_abstractive_reader_outputs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_fused.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonlfused_w_abs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_gen_reranked_reader_outputs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_preprocessed_queryenc_wikipassages_NQ_open_val_<class 'transformers.tokenization_roberta.RobertaTokenizer'>.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_ranked_outputs.jsonl\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reader_outputs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reranked_outputs.jsonl\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reranked_outputs.jsonl_fuseindecoder_preprocessed_for_C25_t5-base_L512.json\n"
     ]
    }
   ],
   "source": [
    "!ls \"/mnt/athena19/opt/effqa_full/efficientQA_submission\" | grep 'NQ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "\n",
    "prefix = \"\"\n",
    "#prefix = \"/mnt/athena19/opt/effqa_full/efficientQA_submission/\"\n",
    "\n",
    "#extractive_reader_output_f = prefix + \"NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reader_outputs.json\"\n",
    "extractive_reader_output_f = prefix + \"NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_fused.json\"\n",
    "ranker_predictions=prefix +\"NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_ranked_outputs.jsonl\"\n",
    "reranker_predictions=prefix +\"NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reranked_outputs.jsonl\"\n",
    "\n",
    "\n",
    "gt_file = \"efficientqa_eval/NQ-open.efficientqa.dev.1.1.jsonl\"\n",
    "\n",
    "with open(extractive_reader_output_f, encoding=\"utf8\") as f:\n",
    "    extractive_reader_predictions = json.load(f)\n",
    "with jsonlines.open(ranker_predictions) as rf:    \n",
    "    ranked_predictions = list(rf)\n",
    "with jsonlines.open(reranker_predictions) as rrf:    \n",
    "    reranked_predictions = list(rrf)\n",
    "\n",
    "inference_type=\"hardmax\" # hardmax / marginal\n",
    "\n",
    "MAX_ANS = 50\n",
    "    \n",
    "import jsonlines\n",
    "correct = {x['question']:x['answer'] for x in list(jsonlines.open(gt_file))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of extractive answers: 50\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum number of extractive answers: {len(extractive_reader_predictions['0']['answers'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "1800\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(extractive_reader_predictions))\n",
    "print(len(ranked_predictions))\n",
    "print(len(reranked_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'question', 'predicted_indices', 'predicted_scores'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_predictions[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fit_qa.scripts.common.evaluate_predictions as eval_utils\n",
    "from fit_qa.scripts.common.evaluate_predictions import exact_match_score\n",
    "\n",
    "labels = []\n",
    "ext_scores, ranker_scores, reranker_scores = [], [], []\n",
    "\n",
    "assert len(extractive_reader_predictions.values()) == len(ranked_predictions) == len(reranked_predictions)\n",
    "for e_p, ranked_p, reranked_p in zip(extractive_reader_predictions.values(),ranked_predictions,reranked_predictions):\n",
    "    assert e_p['raw_question'] == ranked_p['question'] == reranked_p['question']\n",
    "    correct_answers = correct[e_p['raw_question']]\n",
    "    \n",
    "    # Get ground truth rank, if there is gt\n",
    "    gt_rank = -1 \n",
    "    for i, a in enumerate(e_p['answers']):\n",
    "        if eval_utils.metric_max_over_ground_truths(\n",
    "            metric_fn=exact_match_score, prediction=a, ground_truths=correct_answers):\n",
    "            gt_rank = i\n",
    "            break\n",
    "    if gt_rank <0:\n",
    "        continue\n",
    "    assert len(e_p['reader_scores'])==MAX_ANS\n",
    "    labels.append(gt_rank)\n",
    "    \n",
    "    # Get ranker and reranker scores for each answer's passage\n",
    "    answer_passages = e_p['passages']\n",
    "    \n",
    "   \n",
    "    rs = [ranked_p['predicted_scores'][ranked_p['predicted_indices'].index(passage_idx)] \n",
    "                     for passage_idx in answer_passages]\n",
    "    rrs = [reranked_p['predicted_scores'][reranked_p['predicted_indices'].index(passage_idx)] \n",
    "                     for passage_idx in answer_passages]\n",
    "    ext_scores.append(e_p['reader_scores'])\n",
    "    ranker_scores.append(rs)\n",
    "    reranker_scores.append(rrs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_scores_test, ranker_scores_test, reranker_scores_test = [], [], []\n",
    "\n",
    "for e_p, ranked_p, reranked_p in zip(extractive_reader_predictions.values(),ranked_predictions,reranked_predictions):\n",
    "    assert e_p['raw_question'] == ranked_p['question'] == reranked_p['question']\n",
    "       \n",
    "    # Get ranker and reranker scores for each answer's passage\n",
    "    answer_passages = e_p['passages']\n",
    "    \n",
    "   \n",
    "    rs = [ranked_p['predicted_scores'][ranked_p['predicted_indices'].index(passage_idx)] \n",
    "                     for passage_idx in answer_passages]\n",
    "    rrs = [reranked_p['predicted_scores'][reranked_p['predicted_indices'].index(passage_idx)] \n",
    "                     for passage_idx in answer_passages]\n",
    "    ext_scores_test.append(e_p['reader_scores'])\n",
    "    ranker_scores_test.append(rs)\n",
    "    reranker_scores_test.append(rrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "labels = torch.LongTensor(labels)\n",
    "X = torch.FloatTensor(ext_scores)\n",
    "Y = torch.FloatTensor(ranker_scores)\n",
    "Z = torch.FloatTensor(reranker_scores)\n",
    "X_test = torch.FloatTensor(ext_scores_test)\n",
    "Y_test = torch.FloatTensor(ranker_scores_test)\n",
    "Z_test = torch.FloatTensor(reranker_scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1240, 50]),\n",
       " torch.Size([1240, 50]),\n",
       " torch.Size([1240, 50]),\n",
       " torch.Size([1800, 50]),\n",
       " torch.Size([1800, 50]),\n",
       " torch.Size([1800, 50]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape, Z.shape , X_test.shape, Y_test.shape, Z_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class ConstrainedLR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn((1), requires_grad=True))\n",
    "        self.b = torch.nn.Parameter(torch.randn((1), requires_grad=True))\n",
    "        self.c = torch.nn.Parameter(torch.randn((1), requires_grad=True))\n",
    "        self.bias = torch.nn.Parameter(torch.randn((1), requires_grad=True))\n",
    "        #top-k prior for each position\n",
    "        #self.bias = torch.nn.Parameter(torch.randn((50), requires_grad=True))\n",
    "        \n",
    "    def forward(self,X,Y,Z):\n",
    "        r = X*self.a +  Y*self.b + Z * self.c + self.bias \n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds, gts):\n",
    "    assert len(preds) == len(gts)\n",
    "    hits = 0\n",
    "    for q, a in preds.items():\n",
    "        gt_answers = gts[q]\n",
    "        hits+= int(eval_utils.metric_max_over_ground_truths(\n",
    "            metric_fn=exact_match_score, prediction=a, ground_truths=gt_answers))\n",
    "    acc = hits*100./len(preds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(S):\n",
    "    fused_predictions = dict()\n",
    "#     if not S.shape[0] == len(extractive_reader_predictions):\n",
    "#         print(S.shape)\n",
    "#         print(len(extractive_reader_predictions))\n",
    "    assert S.shape[0] == len(extractive_reader_predictions)\n",
    "    for i, (e_p, ranked_p, reranked_p) in enumerate(zip(extractive_reader_predictions.values(), ranked_predictions, reranked_predictions)):\n",
    "#         if not e_p['raw_question'] == ranked_p['question'] == reranked_p['question']:\n",
    "#             print(e_p['raw_question'])\n",
    "#             print(ranked_p['question'])\n",
    "#             print(reranked_p['question'])\n",
    "        assert e_p['raw_question'] == ranked_p['question'] == reranked_p['question']\n",
    "        fused_scores= S[i].tolist()\n",
    "        def argmax(l):\n",
    "            f = lambda i: l[i]\n",
    "            return max(range(len(l)), key=f)\n",
    "        fused_predictions[e_p[\"raw_question\"]] =  e_p[\"answers\"][argmax(fused_scores)]\n",
    "    return evaluate(fused_predictions,correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47.44444444444444: 100%|██████████| 100/100 [00:06<00:00, 16.36it/s]\n",
      "46.5:   2%|▏         | 2/100 [00:00<00:06, 14.54it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [0.4434039890766144], 'b': [0.08466459810733795], 'c': [0.019445449113845825], 'bias': [1.1867865324020386]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47.44444444444444: 100%|██████████| 100/100 [00:06<00:00, 16.20it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 16.96it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.49it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.59it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.72it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.64it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.59it/s]\n",
      "47.5: 100%|██████████| 100/100 [00:05<00:00, 17.78it/s]             \n",
      "45.333333333333336:   2%|▏         | 2/100 [00:00<00:06, 14.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [0.42799437046051025], 'b': [0.025308959186077118], 'c': [0.09631370007991791], 'bias': [0.9029167890548706]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.47it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.37it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.12it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.26it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:06<00:00, 15.90it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.01it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:06<00:00, 15.08it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:07<00:00, 13.84it/s]\n",
      "47.5: 100%|██████████| 100/100 [00:06<00:00, 16.16it/s]            \n",
      "47.44444444444444: 100%|██████████| 100/100 [00:05<00:00, 17.31it/s]\n",
      "47.44444444444444: 100%|██████████| 100/100 [00:06<00:00, 16.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "def run_training():\n",
    "    STEPS=100\n",
    "    model =  ConstrainedLR()\n",
    "    model = model.train()\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=0.3)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "                    opt,\n",
    "                    num_warmup_steps=10,\n",
    "                    num_training_steps=STEPS\n",
    "                )\n",
    "\n",
    "    best_acc= 0\n",
    "    best_r = None\n",
    "    iterator = tqdm(range(STEPS))\n",
    "    for i in iterator:\n",
    "        logits = model(X, Y, Z)\n",
    "        l_list = F.cross_entropy(logits, labels, reduction='none')\n",
    "        l = l_list.mean()\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        opt.zero_grad()\n",
    "        if i % 1 == 0:\n",
    "            r = { k: v.tolist() for k,v in dict(model.named_parameters()).items()}\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                acc = evaluate_model(model(X_test, Y_test, Z_test))\n",
    "            model.train()\n",
    "            if acc>best_acc:\n",
    "                iterator.set_description(str(acc))\n",
    "                best_acc = acc\n",
    "                best_r = r\n",
    "    return best_acc, best_r\n",
    "total_best_acc, total_best_r = 0.,None\n",
    "for _ in range(20):\n",
    "    acc,r = run_training()\n",
    "    if acc>total_best_acc:\n",
    "        print(r)\n",
    "        total_best_acc=acc\n",
    "        total_best_r=r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g.\n",
    "```\n",
    "{'a': [0.9799807071685791], 'b': [0.23300687968730927], 'c': [0.1416887491941452], 'bias': [0.802649199962616]}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 47.666666666666664\n",
      "Best parameters:\n",
      "{'a': [0.44447702169418335], 'b': [0.032241690903902054], 'c': [0.11667594313621521], 'bias': [0.5796049237251282]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best accuracy: {total_best_acc}\")\n",
    "print(f\"Best parameters:\\n{total_best_r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.55555555555556"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only extractive reader\n",
    "model =  ConstrainedLR()\n",
    "model = model.eval()\n",
    "model.a[0] = 1.\n",
    "model.b[0] = 0.\n",
    "model.c[0] = 0.\n",
    "model.bias[0] = 0.\n",
    "evaluate_model(model(X_test, Y_test, Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.111111111111114"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only ranker\n",
    "model =  ConstrainedLR()\n",
    "model = model.eval()\n",
    "model.a[0] = 0.\n",
    "model.b[0] = 1.\n",
    "model.c[0] = 0.\n",
    "model.bias[0] = 0.\n",
    "evaluate_model(model(X_test, Y_test, Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.27777777777778"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only reranker\n",
    "model =  ConstrainedLR()\n",
    "model = model.eval()\n",
    "model.a[0] = 0.\n",
    "model.b[0] = 0.\n",
    "model.c[0] = 1.\n",
    "model.bias[0] = 0.\n",
    "evaluate_model(model(X_test, Y_test, Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.833333333333336"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use nothing (sanity check)\n",
    "model =  ConstrainedLR()\n",
    "model = model.eval()\n",
    "model.a[0] = 0.\n",
    "model.b[0] = 0.\n",
    "model.c[0] = 0.\n",
    "model.bias[0] = 0.\n",
    "evaluate_model(model(X_test, Y_test, Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = model(X_test, Y_test, Z_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
