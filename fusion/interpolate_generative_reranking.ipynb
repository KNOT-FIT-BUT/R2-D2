{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ifajcik/PycharmProjects/efficientQA'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_abstractive_reader_outputs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_fused.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_gen_reranked_reader_outputs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_preprocessed_queryenc_wikipassages_NQ_open_val_<class 'transformers.tokenization_roberta.RobertaTokenizer'>.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_ranked_outputs.jsonl\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reader_outputs.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reranked_outputs.jsonl\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reranked_outputs.jsonl_fuseindecoder_preprocessed_for_C25_t5-base_L512.json\n",
      "NQ-open.efficientqa.dev.1.1.no-annotations.jsonlfused_w_abs.json\n"
     ]
    }
   ],
   "source": [
    "! ls | grep 'NQ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "extractive_reader_output_f = \"NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_reader_outputs.json\"\n",
    "genreranked_reader_predictions_f  =\"NQ-open.efficientqa.dev.1.1.no-annotations.jsonl_gen_reranked_reader_outputs.json\"\n",
    "\n",
    "\n",
    "gt_file = \"efficientqa_eval/NQ-open.efficientqa.dev.1.1.jsonl\"\n",
    "\n",
    "with open(genreranked_reader_predictions_f, encoding=\"utf8\") as f:\n",
    "    genreranked_reader_predictions =  json.load(f)\n",
    "with open(extractive_reader_output_f, encoding=\"utf8\") as f:\n",
    "    extractive_reader_predictions = json.load(f)\n",
    "\n",
    "\n",
    "inference_type=\"hardmax\" # hardmax / marginal\n",
    "\n",
    "MAX_ANS = 50\n",
    "    \n",
    "import jsonlines\n",
    "correct = {x['question']:x['answer'] for x in list(jsonlines.open(gt_file))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of fused answers: 50\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum number of fused answers: {len(extractive_reader_predictions['0']['answers'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative re-ranking only  filters answer of whitespacespan shorter than 10, remove this when martin fixes answer lengts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "genreranked_reader_predictions_trunc=dict()\n",
    "for k,v in genreranked_reader_predictions.items():\n",
    "    g_p = copy.deepcopy(v)\n",
    "    g_p ['reader_scores'] = g_p ['reader_scores'][:MAX_ANS]\n",
    "    g_p ['answers'] = g_p ['answers'][:MAX_ANS]\n",
    "    \n",
    "    genreranked_reader_predictions_trunc[k]= g_p\n",
    "genreranked_reader_predictions = genreranked_reader_predictions_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "genreranked_reader_predictions_corrected = dict()\n",
    "# Deduplicate\n",
    "for k,v, in genreranked_reader_predictions.items():\n",
    "    g_p = copy.deepcopy(v)\n",
    "    scores , answers= [], []\n",
    "    for a,s in zip(v['answers'], v[\"reader_scores\"]):\n",
    "            if not a in answers: \n",
    "                scores.append(s)\n",
    "                answers.append(a)\n",
    "            \n",
    "    g_p ['reader_scores'] = scores\n",
    "    g_p ['answers'] = answers\n",
    "    genreranked_reader_predictions_corrected[k] = g_p\n",
    "genreranked_reader_predictions = genreranked_reader_predictions_corrected\n",
    "    \n",
    "# Deduplicate and keep only ones that are in generative\n",
    "extractive_reader_predictions_corrected = dict()\n",
    "for k,v in extractive_reader_predictions.items():\n",
    "    e_p = copy.deepcopy(v)\n",
    "    # Dirty heuristic filtering, TODO: remove when Martin fixes answer length filtering\n",
    "    scores , answers= [], []\n",
    "    for a,s in zip(v['answers'], v[\"reader_scores\"]):\n",
    "        if a in genreranked_reader_predictions[k]['answers']:\n",
    "            if a in answers:\n",
    "                if inference_type == \"marginal\":\n",
    "                    idx = answers.index(a)\n",
    "                    scores[idx]+= math.exp(s) # add score of this answer span\n",
    "            else:\n",
    "                scores.append(math.exp(s))\n",
    "                answers.append(a)  \n",
    "    e_p ['reader_scores'] = [math.log(s) for s in scores]\n",
    "    e_p ['answers'] = answers\n",
    "    extractive_reader_predictions_corrected[k]= e_p\n",
    "    \n",
    "extractive_reader_predictions = extractive_reader_predictions_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all answers are the same\n",
    "for i, (e_p, g_p) in enumerate(zip(extractive_reader_predictions.values(), genreranked_reader_predictions.values())):\n",
    "    if not len(g_p[\"answers\"]) == len(e_p[\"answers\"]):\n",
    "        print(i)\n",
    "        print(g_p[\"answers\"])\n",
    "        print(g_p[\"reader_scores\"])\n",
    "        \n",
    "        print(e_p[\"answers\"])\n",
    "    assert len(g_p[\"answers\"]) == len(e_p[\"answers\"])\n",
    "    for ag, ae in zip(g_p[\"answers\"],e_p[\"answers\"]):\n",
    "            if not ag == ae:\n",
    "                print(g_p[\"answers\"],e_p[\"answers\"])\n",
    "            assert ag == ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get only predictions, which contains correct answer in top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fit_qa.scripts.common.evaluate_predictions as eval_utils\n",
    "from fit_qa.scripts.common.evaluate_predictions import exact_match_score\n",
    "labels,X,Y = [], [], [] \n",
    "\n",
    "for e_p, g_p in zip(extractive_reader_predictions.values(), genreranked_reader_predictions.values()):\n",
    "    assert e_p['raw_question'] == g_p['raw_question']\n",
    "    correct_answers = correct[e_p['raw_question']]\n",
    "    gt_rank = -1 \n",
    "    for i, a in enumerate(g_p['answers']):\n",
    "        if eval_utils.metric_max_over_ground_truths(\n",
    "            metric_fn=exact_match_score, prediction=a, ground_truths=correct_answers):\n",
    "            gt_rank = i\n",
    "            break\n",
    "    if gt_rank <0:\n",
    "        continue\n",
    "    labels.append(gt_rank)\n",
    "    \n",
    "    if len(g_p['reader_scores'])<MAX_ANS:\n",
    "        g_p['reader_scores'] += [-10_000]*(MAX_ANS - len(g_p['reader_scores']))\n",
    "    if len(e_p['reader_scores'])<MAX_ANS:\n",
    "        e_p['reader_scores'] += [-10_000]*(MAX_ANS - len(e_p['reader_scores']))\n",
    "    \n",
    "    X.append(e_p['reader_scores'])\n",
    "    Y.append(g_p['reader_scores'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fit_qa.scripts.common.evaluate_predictions as eval_utils\n",
    "from fit_qa.scripts.common.evaluate_predictions import exact_match_score\n",
    "\n",
    "X_test,Y_test = [], [] \n",
    "\n",
    "for e_p, g_p in zip(extractive_reader_predictions.values(), genreranked_reader_predictions.values()):\n",
    "    assert e_p['raw_question'] == g_p['raw_question']\n",
    "    \n",
    "    if len(g_p['reader_scores'])<MAX_ANS:\n",
    "        g_p['reader_scores'] += [-10_000]*(MAX_ANS - len(g_p['reader_scores']))\n",
    "    if len(e_p['reader_scores'])<MAX_ANS:\n",
    "        e_p['reader_scores'] += [-10_000]*(MAX_ANS - len(e_p['reader_scores']))\n",
    "    \n",
    "    X_test.append(e_p['reader_scores'])\n",
    "    Y_test.append(g_p['reader_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-4.7501e-04, -2.8762e+01, -2.9989e+01,  ..., -5.5894e+01,\n",
       "          -5.5960e+01, -1.0000e+04],\n",
       "         [-2.3366e-03, -1.6164e+01, -1.7683e+01,  ..., -4.4904e+01,\n",
       "          -1.0000e+04, -1.0000e+04],\n",
       "         [-1.7982e+00, -6.9578e+00, -8.3420e+00,  ..., -1.0000e+04,\n",
       "          -1.0000e+04, -1.0000e+04],\n",
       "         ...,\n",
       "         [-2.0917e+00, -1.7857e+01, -2.1356e+01,  ..., -1.0000e+04,\n",
       "          -1.0000e+04, -1.0000e+04],\n",
       "         [-1.2662e-02, -1.9288e+01, -2.2638e+01,  ..., -1.0000e+04,\n",
       "          -1.0000e+04, -1.0000e+04],\n",
       "         [-4.8160e-05, -2.4338e+01, -2.5361e+01,  ..., -1.0000e+04,\n",
       "          -1.0000e+04, -1.0000e+04]]),\n",
       " tensor([[-1.9833e-01, -4.0924e+00, -1.1933e+01,  ..., -2.1963e+01,\n",
       "          -2.3005e+01, -1.0000e+04],\n",
       "         [-1.1419e-01, -6.9511e+00, -9.5937e+00,  ..., -1.2082e+01,\n",
       "          -1.0000e+04, -1.0000e+04],\n",
       "         [-4.7947e-01, -2.2238e+00, -1.0346e+01,  ..., -1.0000e+04,\n",
       "          -1.0000e+04, -1.0000e+04],\n",
       "         ...,\n",
       "         [-2.9003e+00, -5.0978e+00, -7.7876e+00,  ..., -1.0000e+04,\n",
       "          -1.0000e+04, -1.0000e+04],\n",
       "         [-4.5585e-01, -1.3930e+01, -5.9517e+00,  ..., -1.0000e+04,\n",
       "          -1.0000e+04, -1.0000e+04],\n",
       "         [-4.2633e-01, -1.7057e+01, -1.1616e+01,  ..., -1.0000e+04,\n",
       "          -1.0000e+04, -1.0000e+04]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "labels = torch.LongTensor(labels)\n",
    "X = torch.FloatTensor(X)\n",
    "Y = torch.FloatTensor(Y)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "Y_test = torch.FloatTensor(Y_test)\n",
    "X, Y, labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1224, 50]), torch.Size([1224, 50]), torch.Size([1224]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1800, 50]), torch.Size([1800, 50]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.9409)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "F.cross_entropy(X+Y, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([-5]).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class ConstrainedLR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.λ = torch.nn.Parameter(torch.randn((1), requires_grad=True))\n",
    "        self.α = torch.nn.Parameter(torch.randn((1), requires_grad=True))\n",
    "        \n",
    "    def forward(self,X,Y, mask=None):\n",
    "        r =X*self.α +  Y*self.λ \n",
    "        # I also tried\n",
    "        # torch.exp(X)*self.α + torch.exp(Y)*self.λ \n",
    "        # torch.exp(X*self.α) + torch.exp(Y*self.λ )\n",
    "\n",
    "        if mask is not None:\n",
    "            r[mask]=-float(\"inf\")\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds, gts):\n",
    "    assert len(preds) == len(gts)\n",
    "    hits = 0\n",
    "    for q, a in preds.items():\n",
    "        gt_answers = gts[q]\n",
    "        hits+= int(eval_utils.metric_max_over_ground_truths(\n",
    "            metric_fn=exact_match_score, prediction=a, ground_truths=gt_answers))\n",
    "    acc = hits*100./len(preds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(S):\n",
    "    fused_predictions = dict()\n",
    "    assert S.shape[0] == len(extractive_reader_predictions)\n",
    "    for i, (e_p, g_p) in enumerate(zip(extractive_reader_predictions.values(), genreranked_reader_predictions.values())):\n",
    "        assert e_p['raw_question'] == g_p['raw_question']\n",
    "        fused_scores= S[i].tolist()\n",
    "        def argmax(l):\n",
    "            f = lambda i: l[i]\n",
    "            return max(range(len(l)), key=f)\n",
    "        if len( e_p[\"answers\"])<=argmax(fused_scores):\n",
    "            print(fused_scores)\n",
    "            print(i)\n",
    "            print(e_p[\"answers\"])\n",
    "        fused_predictions[e_p[\"raw_question\"]] =  e_p[\"answers\"][argmax(fused_scores)]\n",
    "    return evaluate(fused_predictions,correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47.0: 100%|██████████| 100/100 [00:06<00:00, 15.30it/s]             \n",
      "42.611111111111114:   2%|▏         | 2/100 [00:00<00:05, 16.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'λ': 0.6657118201255798, 'α': 0.08693467080593109}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47.111111111111114: 100%|██████████| 100/100 [00:06<00:00, 14.66it/s]\n",
      "46.44444444444444:   2%|▏         | 2/100 [00:00<00:05, 16.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'λ': 0.4853821098804474, 'α': 0.04384877532720566}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47.111111111111114: 100%|██████████| 100/100 [00:06<00:00, 15.29it/s]\n",
      "47.0: 100%|██████████| 100/100 [00:06<00:00, 14.59it/s]             \n",
      "47.0: 100%|██████████| 100/100 [00:06<00:00, 14.34it/s]            \n",
      "47.0: 100%|██████████| 100/100 [00:06<00:00, 15.37it/s]             \n",
      "47.05555555555556: 100%|██████████| 100/100 [00:06<00:00, 14.91it/s]\n",
      "47.111111111111114: 100%|██████████| 100/100 [00:06<00:00, 14.35it/s]\n",
      "47.05555555555556: 100%|██████████| 100/100 [00:06<00:00, 14.64it/s]\n",
      "47.111111111111114: 100%|██████████| 100/100 [00:07<00:00, 14.08it/s]\n",
      "47.05555555555556: 100%|██████████| 100/100 [00:07<00:00, 13.72it/s]\n",
      "47.111111111111114: 100%|██████████| 100/100 [00:06<00:00, 14.30it/s]\n",
      "47.166666666666664: 100%|██████████| 100/100 [00:06<00:00, 14.46it/s]\n",
      "45.77777777777778:   0%|          | 0/100 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'λ': 0.5088504552841187, 'α': 0.053242623805999756}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47.111111111111114: 100%|██████████| 100/100 [00:06<00:00, 14.96it/s]\n",
      "47.05555555555556: 100%|██████████| 100/100 [00:06<00:00, 14.50it/s]\n",
      "47.111111111111114: 100%|██████████| 100/100 [00:06<00:00, 14.68it/s]\n",
      "46.94444444444444: 100%|██████████| 100/100 [00:07<00:00, 14.07it/s]\n",
      "47.05555555555556: 100%|██████████| 100/100 [00:06<00:00, 15.02it/s]\n",
      "47.05555555555556: 100%|██████████| 100/100 [00:06<00:00, 15.21it/s]\n",
      "47.111111111111114: 100%|██████████| 100/100 [00:06<00:00, 14.48it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_training():\n",
    "    import torch.nn.functional as F\n",
    "    from tqdm import tqdm\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "    training_mask = (X+Y)<-9999\n",
    "    test_mask = (X_test+Y_test)<-9999\n",
    "    STEPS=100\n",
    "    model =  ConstrainedLR()\n",
    "    model = model.train()\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=0.3)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "                    opt,\n",
    "                    num_warmup_steps=10,\n",
    "                    num_training_steps=STEPS\n",
    "                )\n",
    "\n",
    "    best_acc= 0\n",
    "    best_r = None\n",
    "    iterator = tqdm(range(STEPS))\n",
    "    for i in iterator:\n",
    "        logits = model(X, Y, training_mask)\n",
    "        l_list = F.cross_entropy(logits, labels, reduction='none')\n",
    "        l = l_list.mean()\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        opt.zero_grad()\n",
    "        if i % 1 == 0:\n",
    "            r = { k: v.item() for k,v in dict(model.named_parameters()).items()}\n",
    "            model.eval()\n",
    "            acc = evaluate_model(model(X_test,Y_test, test_mask))\n",
    "            model.train()\n",
    "            if acc>best_acc:\n",
    "                iterator.set_description(str(acc))\n",
    "                best_acc = acc\n",
    "                best_r = r\n",
    "    return best_acc, best_r\n",
    "total_best_acc, total_best_r = 0.,None\n",
    "for _ in range(20):\n",
    "    acc,r = run_training()\n",
    "    if acc>total_best_acc:\n",
    "        print(r)\n",
    "        total_best_acc=acc\n",
    "        total_best_r=r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g.   \n",
    "```\n",
    "Best accuracy: 47.166666666666664\n",
    "Best parameters:\n",
    "{'λ': 0.4845413267612457, 'α': 0.05110572651028633}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 47.166666666666664\n",
      "Best parameters:\n",
      "{'λ': 0.5088504552841187, 'α': 0.053242623805999756}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best accuracy: {total_best_acc}\")\n",
    "print(f\"Best parameters:\\n{total_best_r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  ConstrainedLR()\n",
    "model = model.eval()\n",
    "test_mask = (X_test+Y_test)<-9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_r_sd = { k: torch.FloatTensor([v]) for k,v in total_best_r.items()}\n",
    "model.load_state_dict(best_r_sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.611111111111114"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only extractive reader\n",
    "model.λ[0] = 0.\n",
    "evaluate_model(model(X_test,Y_test, test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_r_sd = { k: torch.FloatTensor([v]) for k,v in total_best_r.items()}\n",
    "model.load_state_dict(best_r_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.22222222222222"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only generative reader (scoring extractive predictions!)\n",
    "model.α[0] = 0.\n",
    "evaluate_model(model(X_test,Y_test, test_mask))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
